import argparse
import random
import torch
import torch.optim as optim
import torch.nn.functional as F
from tqdm import tqdm
import open_clip
from utils.mv_utils_zs import Realistic_Projection
from model.PointNet import PointNetfeat, feature_transform_regularizer
from utils.dataloader_ModelNet40 import *
import os
import numpy as np
from matplotlib import pyplot as plt

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
prompts = ['A three-dimensional model of an airplane composed of gray, fuzzy balls.', 'A lumpy 3D model of a slanted bathtub made of dull gray balls.', 'A bed is typically represented by a rectangular shape on a grayscale map.', 'A bench can be identified from a grayscale map by its shape.', 'An unclear depth map in shades of gray of a bookshelf model at a slanted angle.', 'A bottle generally has a cylindrical shape and a narrow neck.', 'A 3D model of a bowl composed of gray balls that are difficult to see.', 'This is a depth map of a car 3D model, generated by depth sensing cameras.', 'The chair would be a dark object on the grayscale map.', 'A 3D model of a cone would look like a cone shape.', 'An obscure cup was found at the depth map.', 'There is less light in an obscure depth map of a curtain, so the features are not as clearly defined.', 'An obscure depth map of a desk would likely show a few objects on the desk in great detail, while the rest of the desk would be less detailed and more blurry.', 'This sketch depth map shows the door of a room.', 'A white heightmap in a black background of a dresser would look like a white rectangle in the center of the dresser with a black border.', 'A depth map of a flower pot can be identified by its shading.', 'A glass box will appear as a bright white region in a depth map.', 'I am looking at a 3D model of a guitar.', '3D render of a keyboard with heightmap.', 'There is an obscure depth map of a lamp.', 'The depth map of a laptop 3D model is a top-down view of the laptop, showing its various components in different colors.', 'The left or right view depth map of a white mantel would look like a white rectangle with some depth to it.', 'A grayscale image of a monitor.', 'A white, porous depth map of a night stand might look like a ghostly image of the furniture piece, with its contours and dimensions visible but slightly blurred.', 'A heightmap of a person, showing their height at different points along their body.', 'The piano would be the blackest object on the grayscale map.', 'This depth map is of a plant against a black background and is full of pores.', 'A typical radiolooks like a rectangular box with a handle on the top.', 'There is an obscure depth map of a range hood.', 'A depth map of a sink 3D model can be quite obscure, as the sink is often hidden behind other objects in a room.', 'This is a depth map of a 3D model of a sofa.', 'Thestairs3Dmodelhas a Depth Map that is quite Obscure .', 'A stool can vary in shape and size, but typically it is a small, round object that is used for sitting.', 'A 3D model of a table typically looks like a rectangular object with four legs.', 'A grayscale or white depth map of a tent 3D model would show the tent as a white object against a black background.', 'The depth map of a white toilet would appear as a white object with a black outline.', 'The image is a depth map of a 3D model of a TV stand.', 'A grayscale or white depth map of a vase 3D model would show the contours of the vase in shades of gray or white, with the darkest areas representing the deepest parts of the vase.', 'A grayscale or white depth map of a wardrobe 3D model would show the overall shape and form of the wardrobe, as well as the depth of each component.', 'A grayscale depth map of a 3D Xbox model would show a range of light to dark gray tones, with the darkest areas representing the closest parts of the model to the viewer, and the lightest areas representing the farthest parts of the model.']
view_weight = [0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.25, 1.0, 0.75, 0.25]
view_weights = torch.tensor(view_weight).to(device)

# define a function for mathcing feature_2D (512) to Matrix with size (512 * 1000) colomn-wise with cosine similarity
def clip_similarity(image_features, text_features):
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    text_probs = (100 * image_features @ text_features.T).softmax(dim=-1)
    return text_probs

def accuracy(output, target, topk=(1,)):
    pred = output.topk(max(topk), 1, True, True)[1].t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))
    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]

def set_random_seed(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# read a txt file line by line and save it in a list, and remove the empty lines
def read_txt_file(file):
    with open(file, 'r') as f:
        array = f.readlines()
    array = ["An image of a " + x.strip() for x in array]
    array = list(filter(None, array))
    return array

def accuracy(output, target, topk=(1,)):
    pred = output.topk(max(topk), 1, True, True)[1].t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))
    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]

# define the main function
def main(opt):

    set_random_seed(opt.manualSeed) 
    # deine data loader
    train_dataset = ModelNetDataLoader(root='dataset/modelnet40_normal_resampled/', args=opt, split='train', process_data=opt.process_data)
    test_dataset = ModelNetDataLoader(root='dataset/modelnet40_normal_resampled/', args=opt, split='test', process_data=opt.process_data)
    trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=10, drop_last=True)
    testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=10)

    # Step 1: Load CLIP model
    clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion2b_s34b_b88k')
    clip_model.to(device)
    clip_model.eval()

    # Step 2: Load Realistic Projection object
    proj = Realistic_Projection()

    #load the text features
    prompts = read_txt_file("class_name_modelnet40.txt")
    text_features_tmp = torch.zeros((len(prompts), 512), device=device)
    text = open_clip.tokenize(prompts)
    with torch.no_grad(), torch.cuda.amp.autocast():
         text_features = clip_model.encode_text(text.to(device))
           

    # score prediction
    base_class_correct = 0
    base_class_total = 0
    Logits = torch.zeros(2468,40).to(device)
    Target = torch.zeros(2468).to(device)

    for j, data in tqdm(enumerate(testDataLoader, 0)):
        points, target = data
        # convert numpy.int32 to torch.int32
        points, target = points.to(device), target.to(device)
        features_2D = torch.zeros((1, 512), device=device)
        with torch.no_grad():
                # Project samples to an image
                pc_prj = proj.get_img(points)
                pc_img = torch.nn.functional.interpolate(pc_prj, size=(224, 224), mode='bilinear', align_corners=True)
                pc_img = pc_img.to(device)
                
                # Forward samples to the CLIP model
                pc_img = clip_model.encode_image(pc_img).to(device)
                pc_img = pc_img.unsqueeze(0) * view_weights.reshape(1,-1, 1)

                # Average the features
                pc_img_avg = torch.mean(pc_img.squeeze(0), dim=0)
                # Save feature vectors
                features_2D = pc_img_avg

        
        img_features = features_2D
        img_features /= img_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        logits = clip_model.logit_scale.exp() * img_features @ text_features.t().float() * 1.0
        Logits[j] = logits
        Target[j] = target

    acc, _ = accuracy(Logits, Target, topk=(1, 5))
    acc = (acc / Target.shape[0]) * 100
    print(f"=> zero-shot accuracy: {acc:.2f}")
   


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch_size', type=int, default= 1, help='input batch size')
    parser.add_argument('--num_points', type=int, default=2048, help='number of points in each input point cloud')
    parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)
    parser.add_argument('--nepoch', type=int, default=250, help='number of epochs to train for')
    parser.add_argument('--outf', type=str, default='cls', help='output folder to save results')
    parser.add_argument('--model', type=str, default='cls/3D_model_249.pth', help='path to load a pre-trained model')
    parser.add_argument('--feature_transform', action='store_true', help='use feature transform')
    parser.add_argument('--manualSeed', type=int, default = -1, help='random seed')
    parser.add_argument('--dataset_path', type=str, default= 'dataset/modelnet_scanobjectnn/', help="dataset path")
    parser.add_argument('--ntasks', type=str, default= '1', help="number of tasks")
    parser.add_argument('--nclasses', type=str, default= '26', help="number of classes")
    parser.add_argument('--task', type=str, default= '0', help="task number")
    parser.add_argument('--num_samples', type=str, default= '0', help="number of samples per class")
    parser.add_argument('--process_data', action='store_true', default=False, help='save data offline')
    parser.add_argument('--num_point', type=int, default=2048, help='Point Number')
    parser.add_argument('--use_uniform_sample', action='store_true', default=False, help='use uniform sampiling')
    parser.add_argument('--use_normals', action='store_true', default=False, help='use normals')
    parser.add_argument('--num_category', default=40, type=int, choices=[10, 40],  help='training on ModelNet10/40')
    opt = parser.parse_args()
    main(opt)
    print("Done!")